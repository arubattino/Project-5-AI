{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2etesk9je0X"
   },
   "source": [
    "# Video Classification with Transformers\n",
    "\n",
    "**Model based in:** Sayak Paul<br>\n",
    "\n",
    "validation_accuracy result: 38%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 18 21:43:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeIm3pdnje0g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:41:51.529719Z",
     "iopub.status.busy": "2022-07-17T15:41:51.528487Z",
     "iopub.status.idle": "2022-07-17T15:41:57.728103Z",
     "shell.execute_reply": "2022-07-17T15:41:57.727103Z",
     "shell.execute_reply.started": "2022-07-17T15:41:51.529666Z"
    },
    "id": "0iJpe-yFje0h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import x509\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB04eHICje0i"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:51:39.455978Z",
     "iopub.status.busy": "2022-07-17T15:51:39.455294Z",
     "iopub.status.idle": "2022-07-17T15:51:39.460723Z",
     "shell.execute_reply": "2022-07-17T15:51:39.459759Z",
     "shell.execute_reply.started": "2022-07-17T15:51:39.455944Z"
    },
    "id": "cdhMUE8Vje0i"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 8\n",
    "NUM_FEATURES = 2048\n",
    "IMG_SIZE = 224\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPCw-l-Lje0j"
   },
   "source": [
    "## Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:42:04.433591Z",
     "iopub.status.busy": "2022-07-17T15:42:04.432746Z",
     "iopub.status.idle": "2022-07-17T15:42:04.483894Z",
     "shell.execute_reply": "2022-07-17T15:42:04.482888Z",
     "shell.execute_reply.started": "2022-07-17T15:42:04.433551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 15844\n",
      "Total videos for testing: 196\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"finish_data_train.csv\")\n",
    "test_df = pd.read_csv(\"finish_data_test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:49:21.108967Z",
     "iopub.status.busy": "2022-07-17T15:49:21.108243Z",
     "iopub.status.idle": "2022-07-17T15:49:21.118519Z",
     "shell.execute_reply": "2022-07-17T15:49:21.117442Z",
     "shell.execute_reply.started": "2022-07-17T15:49:21.108928Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_center(frame):\n",
    "    cropped = center_crop_layer(frame[None, ...])\n",
    "    cropped = cropped.numpy().squeeze()\n",
    "    return cropped\n",
    "\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=MAX_SEQ_LENGTH):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    # Total frames\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    n_frames = round(length/max_frames)\n",
    "    \n",
    "    total = 0\n",
    "    frames = []\n",
    "    i=0\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or total == max_frames:\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if i%n_frames == 0:\n",
    "                frame = crop_center(frame)\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "                frames.append(frame)\n",
    "                total += 1\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "        if total < max_frames:\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            for j in range(int(length)):\n",
    "                ret, frame = cap.read()\n",
    "                if j == (int(length)-1):\n",
    "                    frame = crop_center(frame)\n",
    "                    frame = frame[:, :, [2, 1, 0]]\n",
    "                    frames.append(frame)\n",
    "            \n",
    "            \n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:49:48.983886Z",
     "iopub.status.busy": "2022-07-17T15:49:48.983244Z",
     "iopub.status.idle": "2022-07-17T15:50:00.762230Z",
     "shell.execute_reply": "2022-07-17T15:50:00.761290Z",
     "shell.execute_reply.started": "2022-07-17T15:49:48.983853Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.Xception(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.xception.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:50:31.034865Z",
     "iopub.status.busy": "2022-07-17T15:50:31.034514Z",
     "iopub.status.idle": "2022-07-17T15:50:31.083230Z",
     "shell.execute_reply": "2022-07-17T15:50:31.082203Z",
     "shell.execute_reply.started": "2022-07-17T15:50:31.034835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 128, 128, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 128, 128, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:50:39.927889Z",
     "iopub.status.busy": "2022-07-17T15:50:39.927533Z",
     "iopub.status.idle": "2022-07-17T15:50:39.972050Z",
     "shell.execute_reply": "2022-07-17T15:50:39.971043Z",
     "shell.execute_reply.started": "2022-07-17T15:50:39.927860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motion', 'Pull', 'Push', 'Static']\n"
     ]
    }
   ],
   "source": [
    "# Label preprocessing with StringLookup.\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"movement_label\"]), mask_token=None\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:51:54.755976Z",
     "iopub.status.busy": "2022-07-17T15:51:54.755631Z",
     "iopub.status.idle": "2022-07-17T15:51:54.769744Z",
     "shell.execute_reply": "2022-07-17T15:51:54.768786Z",
     "shell.execute_reply.started": "2022-07-17T15:51:54.755947Z"
    },
    "id": "ZsqQ_nFsje0k"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"dir\"].values.tolist()\n",
    "    labels = df[\"movement_label\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_features` are what we will feed to our sequence model.\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in tqdm(enumerate(video_paths), desc='Processing T:15844/196'):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholder to store the features of the current video.\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                if np.mean(batch[j, :]) > 0.0:\n",
    "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                        batch[None, j, :]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    temp_frame_features[i, j, :] = 0.0\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "\n",
    "    return frame_features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:52:00.665146Z",
     "iopub.status.busy": "2022-07-17T15:52:00.664684Z",
     "iopub.status.idle": "2022-07-17T16:00:57.581361Z",
     "shell.execute_reply": "2022-07-17T16:00:57.580410Z",
     "shell.execute_reply.started": "2022-07-17T15:52:00.665101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total 15844/196: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7e86362835ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_all_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_all_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Frame features in train set: {train_data[0].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Frame masks in train set: {train_data[1].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c107727489ed>\u001b[0m in \u001b[0;36mprepare_all_videos\u001b[0;34m(df, root_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     temp_frame_features[i, j, :] = feature_extractor.predict(\n\u001b[0m\u001b[1;32m     40\u001b[0m                         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZHdFHeHje0m"
   },
   "source": [
    "## Building the Transformer-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T16:01:26.803522Z",
     "iopub.status.busy": "2022-07-17T16:01:26.803150Z",
     "iopub.status.idle": "2022-07-17T16:01:26.811805Z",
     "shell.execute_reply": "2022-07-17T16:01:26.810543Z",
     "shell.execute_reply.started": "2022-07-17T16:01:26.803490Z"
    },
    "id": "475ImRLDje0m"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrvARnDbje0n"
   },
   "source": [
    "Now, we can create a subclassed layer for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T16:01:32.106964Z",
     "iopub.status.busy": "2022-07-17T16:01:32.106296Z",
     "iopub.status.idle": "2022-07-17T16:01:32.118338Z",
     "shell.execute_reply": "2022-07-17T16:01:32.117168Z",
     "shell.execute_reply.started": "2022-07-17T16:01:32.106929Z"
    },
    "id": "Aaw5Dbf4je0n"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_UPDcrdje0n"
   },
   "source": [
    "## Utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T16:01:38.101384Z",
     "iopub.status.busy": "2022-07-17T16:01:38.101026Z",
     "iopub.status.idle": "2022-07-17T16:01:38.112310Z",
     "shell.execute_reply": "2022-07-17T16:01:38.110595Z",
     "shell.execute_reply.started": "2022-07-17T16:01:38.101341Z"
    },
    "id": "haRDkJk2je0o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_compiled_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = len(label_processor.get_vocabulary())\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None))\n",
    "\n",
    "    x = PositionalEmbedding(sequence_length, embed_dim, name=\"frame_position_embedding\")(inputs)\n",
    "\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    \n",
    "    x = layers.GlobalMaxPooling1D(data_format=\"channels_last\", keepdims=False,)(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001, epsilon=0.1), \n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifierF\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    model = get_compiled_model()\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_split=0.15,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUimY-A9je0o"
   },
   "source": [
    "## Model training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T16:01:42.695379Z",
     "iopub.status.busy": "2022-07-17T16:01:42.694993Z",
     "iopub.status.idle": "2022-07-17T16:01:55.284076Z",
     "shell.execute_reply": "2022-07-17T16:01:55.283050Z",
     "shell.execute_reply.started": "2022-07-17T16:01:42.695340Z"
    },
    "id": "JXaOicg_je0o"
   },
   "outputs": [],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T16:04:05.094383Z",
     "iopub.status.busy": "2022-07-17T16:04:05.094028Z",
     "iopub.status.idle": "2022-07-17T16:04:05.904312Z",
     "shell.execute_reply": "2022-07-17T16:04:05.903413Z",
     "shell.execute_reply.started": "2022-07-17T16:04:05.094341Z"
    },
    "id": "_moZ7OUWje0p"
   },
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Pad shorter videos.\n",
    "    if len(frames) < MAX_SEQ_LENGTH:\n",
    "        diff = MAX_SEQ_LENGTH - len(frames)\n",
    "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "        frames = np.concatenate(frames, padding)\n",
    "\n",
    "    frames = frames[None, ...]\n",
    "\n",
    "    # Extract features from the frames of the current video.\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            if np.mean(batch[j, :]) > 0.0:\n",
    "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "            else:\n",
    "                frame_features[i, j, :] = 0.0\n",
    "\n",
    "    return frame_features\n",
    "\n",
    "\n",
    "def predict_action(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features = prepare_single_video(frames)\n",
    "    probabilities = trained_model.predict(frame_features)[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "test_video = np.random.choice(test_df[\"dir\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = predict_action(test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test analisys\n",
    "\n",
    "asd = test_df.sample(n=5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "mo=0\n",
    "pul=0\n",
    "pus=0\n",
    "sta=0\n",
    "no=0\n",
    "for j in tqdm(range(len(test_df)), desc='Loop'):\n",
    "#for j in range(5):\n",
    "  path = test_df['dir'][j]\n",
    "\n",
    "  val=str(test_df['movement_label'][j])\n",
    "\n",
    "\n",
    "  class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "  frames = load_video(path)\n",
    "  frame_features = prepare_single_video(frames)\n",
    "  probabilities = trained_model.predict(frame_features)[0]\n",
    "  li=[]\n",
    "  for i in np.argsort(probabilities)[::-1]:\n",
    "      li.append({class_vocab[i]})\n",
    "  res=str(li[0])\n",
    "  res=str(res[2:len(res)-2])\n",
    "  \n",
    " \n",
    "  if res == 'Motion' and res == val:\n",
    "    mo+=1\n",
    "\n",
    "  elif res == 'Pull' and res == val:\n",
    "    pul+=1\n",
    "\n",
    "  elif res == 'Push' and res == val:\n",
    "    pus+=1\n",
    "\n",
    "  elif res == 'Static' and res == val:\n",
    "    sta+=1\n",
    "\n",
    "  else:\n",
    "    no+=1\n",
    "\n",
    "print()\n",
    "print(f'Motion: {mo}/50.', (mo*50)/100,\"%\")\n",
    "print(f'Pull: {pul}/46.', (pul*46)/100,\"%\")\n",
    "print(f'Push: {pus}/50.', (pus*50)/100,\"%\")\n",
    "print(f'Static: {sta}/50.', (sta*50)/100,\"%\")\n",
    "print('None: ', no, (no*196)/100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c75407a027d59d8279fdc80f39ae7e88eaf5822626513196156b9ba3e7422158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
