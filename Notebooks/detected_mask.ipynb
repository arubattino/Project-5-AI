{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected Mask:\n",
    "\n",
    "In this notebook, a code was investigated and tested which allows extracting the masks of each video clip. This way the ML model doesn't see the whole clip, but only the subject and background masks. This helps the model receive less information and not confuse the classes. Pytorch and Panopticapi are used.\n",
    "\n",
    "The results of this script can be found at:\n",
    "\n",
    "Video: video_example/video1.mp4\n",
    "\n",
    "Result: video_example/video1_result.mp4\n",
    "\n",
    "________\n",
    "\n",
    "Video: video_example/video2.mp4\n",
    "\n",
    "Result: video_example/video2_result.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.0+cu111 in /home/arubattino/.local/lib/python3.6/site-packages (1.10.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.11.0+cu111 in /home/arubattino/.local/lib/python3.6/site-packages (0.11.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.10.0 in /home/arubattino/.local/lib/python3.6/site-packages (0.10.0+rocm4.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.10.0+cu111) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/arubattino/.local/lib/python3.6/site-packages (from torch==1.10.0+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.11.0+cu111) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.11.0+cu111) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f \\\n",
    "    https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-7us6nuzm because the default path (/home/arubattino/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
      "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-rv1wwa95\n",
      "  Running command git clone --filter=blob:none -q https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-rv1wwa95\n",
      "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from panopticapi==0.1) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from panopticapi==0.1) (8.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/arubattino/.local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as T\n",
    "import numpy\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "! pip install git+https://github.com/cocodataset/panopticapi.git\n",
    "! pip install seaborn\n",
    "\n",
    "import panopticapi\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/arubattino/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    }
   ],
   "source": [
    "# These are the COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# Detectron2 uses a different numbering scheme, we build a conversion table\n",
    "coco2d2 = {}\n",
    "count = 0\n",
    "for i, c in enumerate(CLASSES):\n",
    "  if c != \"N/A\":\n",
    "    coco2d2[i] = count\n",
    "    count+=1\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "model, postprocessor = torch.hub.load('facebookresearch/detr', 'detr_resnet101_panoptic', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mask_frame(path):\n",
    "\n",
    "  ''' \n",
    "      mask_frame() identifies the different masks of an image and replaces it with the original stored image.\n",
    "      path: str: image address to identify the skins (.jpg / .png / etc) \n",
    "  '''\n",
    "  \n",
    "  im = Image.open(path)\n",
    "\n",
    "  # mean-std normalize the input image (batch-size: 1)\n",
    "  img = transform(im).unsqueeze(0)\n",
    "  out = model(img)\n",
    "\n",
    "  # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "  result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
    "\n",
    "  palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "  # The segmentation is stored in a special-format png\n",
    "  panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "  panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8).copy()\n",
    "  # We retrieve the ids corresponding to each mask\n",
    "  panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "\n",
    "  # Finally we color each mask individually\n",
    "  panoptic_seg[:, :, :] = 0\n",
    "  for id in range(panoptic_seg_id.max() + 1):\n",
    "    panoptic_seg[panoptic_seg_id == id] = numpy.asarray(next(palette)) * 255\n",
    "  plt.figure(figsize=(25,15));\n",
    "  plt.imshow(panoptic_seg);\n",
    "  plt.axis('off');\n",
    "  plt.savefig(path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to convert video to video maskara\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_reverse(pathing, save, name):\n",
    "\n",
    "    '''pathing: str: direction of the video to invert\n",
    "       save:    str: address to save the generated video\n",
    "       name:    str: file name '''\n",
    "    \n",
    "    def walkdir(folder):\n",
    "        ''' Directories function '''\n",
    "        for dirpath, _, files in os.walk(folder):\n",
    "            for filename in files:\n",
    "                yield (dirpath, filename)\n",
    "\n",
    "\n",
    "    # Generate the video frames in the probe folder\n",
    "    cap = cv2.VideoCapture(pathing) \n",
    "    check , vid = cap.read() \n",
    "    counter = 0\n",
    "    check = True\n",
    "    frame_list = [] \n",
    "    \n",
    "    while(check == True): \n",
    "        \n",
    "        cv2.imwrite(\"./probe/0000%d.jpg\" %counter , vid) \n",
    "        check , vid = cap.read() \n",
    "        \n",
    "        frame_list.append(vid) \n",
    "        counter += 1\n",
    "    \n",
    "    frame_list.pop()\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Empty array\n",
    "    img_array = []\n",
    "\n",
    "    # Generate list with frames path\n",
    "    f = r'./probe/'\n",
    "    li=[]\n",
    "    for dir,filename in walkdir(f):\n",
    "        li.append(dir+filename)\n",
    "\n",
    "    # Generate a new list in reverse order\n",
    "    list_in_reverse=[]    \n",
    "    for q in range(len(li)):\n",
    "        \n",
    "        mas=0\n",
    "        for j in range(len(li)):\n",
    "            n=int(li[j][len(li[j])-8:len(li[j])-4])\n",
    "            \n",
    "            if n >= mas:\n",
    "                mas = n\n",
    "                r = li[j]\n",
    "        list_in_reverse.append(r)\n",
    "        li.remove(r)\n",
    "    # To reverse the order or not    \n",
    "    list_in_reverse.reverse()\n",
    "\n",
    "    #---------- Frames to mask -----------\n",
    "    for dir, filename in walkdir('./probe'):\n",
    "        pathv=(dir+\"/\"+filename)\n",
    "        mask_frame(pathv)\n",
    "    # --------------------------------------\n",
    "\n",
    "    # \"For\" to read images from a directory\n",
    "    for p in list_in_reverse:\n",
    "        path = p\n",
    "        img = cv2.imread(path)\n",
    "        img_array.append(img)\n",
    "\n",
    "    # Calculate the size of the last image height and width\n",
    "    height, width  = img.shape[:2]\n",
    "    # \"name\" is the name of the file and \"know\" the directory, everything is a \"str\"\n",
    "    # \"24\" indicates the frames per second of the video\n",
    "    video = cv2.VideoWriter(save + name ,cv2.VideoWriter_fourcc(*'mp4v'), 24,(width,height))\n",
    "\n",
    "    # \"For\" to save frames in a video\n",
    "    for i in range(len(img_array)):\n",
    "        video.write(img_array[i])\n",
    "\n",
    "    video.release()  \n",
    "\n",
    "    # Delete the frames from the probe folder\n",
    "    for dir, filename in walkdir('./probe'):\n",
    "        fol=str(dir+'/'+filename)\n",
    "        os.remove(fol)\n",
    "        \n",
    "        \n",
    "video_reverse('tt4275910_shot_0001.mp4', './', 'prueba.mp4');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c75407a027d59d8279fdc80f39ae7e88eaf5822626513196156b9ba3e7422158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
