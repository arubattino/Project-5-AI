{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mefn9zEdblEv"
   },
   "source": [
    "# Video Classification with a CNN-RNN Architecture\n",
    "\n",
    "**Model based in:** Sayak Paul<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:46:06.543643Z",
     "iopub.status.busy": "2022-07-15T13:46:06.543264Z",
     "iopub.status.idle": "2022-07-15T13:46:07.372602Z",
     "shell.execute_reply": "2022-07-15T13:46:07.371419Z",
     "shell.execute_reply.started": "2022-07-15T13:46:06.543613Z"
    },
    "id": "1XgkEQPVbfFr",
    "outputId": "f61eedee-d6f0-4e2f-9d07-430bc863dea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 18 04:37:21 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   45C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qMJZgiMblE3"
   },
   "source": [
    "## Data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:47.453400Z",
     "iopub.status.busy": "2022-07-15T13:47:47.452766Z",
     "iopub.status.idle": "2022-07-15T13:47:59.614863Z",
     "shell.execute_reply": "2022-07-15T13:47:59.613101Z",
     "shell.execute_reply.started": "2022-07-15T13:47:47.453348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/arubattino/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9x0IyBpblE5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:59.617999Z",
     "iopub.status.busy": "2022-07-15T13:47:59.617431Z",
     "iopub.status.idle": "2022-07-15T13:47:59.628665Z",
     "shell.execute_reply": "2022-07-15T13:47:59.627199Z",
     "shell.execute_reply.started": "2022-07-15T13:47:59.617922Z"
    },
    "id": "nMHl0wvTblE5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import x509\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSeq6X_2blE6"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:59.631907Z",
     "iopub.status.busy": "2022-07-15T13:47:59.631326Z",
     "iopub.status.idle": "2022-07-15T13:47:59.639529Z",
     "shell.execute_reply": "2022-07-15T13:47:59.638270Z",
     "shell.execute_reply.started": "2022-07-15T13:47:59.631857Z"
    },
    "id": "8vwV6YUvblE7"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCC8ppddblE7"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:59.643264Z",
     "iopub.status.busy": "2022-07-15T13:47:59.641409Z",
     "iopub.status.idle": "2022-07-15T13:47:59.679020Z",
     "shell.execute_reply": "2022-07-15T13:47:59.677591Z",
     "shell.execute_reply.started": "2022-07-15T13:47:59.643220Z"
    },
    "id": "3kOGt4J7blE8",
    "outputId": "a2685a52-c646-4437-c79d-52d99883242f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 15844\n",
      "Total videos for testing: 196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>movement_label</th>\n",
       "      <th>movement_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8768</th>\n",
       "      <td>./../data/train/tt2293060/shot_0062.mp4</td>\n",
       "      <td>Static</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>./../data/trailer_zoom/tt2527192_shot_0051.mp4</td>\n",
       "      <td>Push</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15303</th>\n",
       "      <td>./../data/train/tt2066041/shot_0001.mp4</td>\n",
       "      <td>Motion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>./../data/train/tt5766118/shot_0036.mp4</td>\n",
       "      <td>Static</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>./../data/trailer_zoom/tt2395421_shot_0030.mp4</td>\n",
       "      <td>Push</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10475</th>\n",
       "      <td>./../data/train/tt2235515/shot_0024.mp4</td>\n",
       "      <td>Static</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>./../data/new_pull/tt3201722_shot_0028.mp4</td>\n",
       "      <td>Pull</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>./../data/new_pull/tt2624704_shot_0010.mp4</td>\n",
       "      <td>Pull</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>./../data/train/tt4151098/shot_0037.mp4</td>\n",
       "      <td>Static</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>./../data/trailer_zoom/tt2140577_shot_0019.mp4</td>\n",
       "      <td>Push</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  dir movement_label  \\\n",
       "8768          ./../data/train/tt2293060/shot_0062.mp4         Static   \n",
       "3960   ./../data/trailer_zoom/tt2527192_shot_0051.mp4           Push   \n",
       "15303         ./../data/train/tt2066041/shot_0001.mp4         Motion   \n",
       "9564          ./../data/train/tt5766118/shot_0036.mp4         Static   \n",
       "3113   ./../data/trailer_zoom/tt2395421_shot_0030.mp4           Push   \n",
       "10475         ./../data/train/tt2235515/shot_0024.mp4         Static   \n",
       "6692       ./../data/new_pull/tt3201722_shot_0028.mp4           Pull   \n",
       "4379       ./../data/new_pull/tt2624704_shot_0010.mp4           Pull   \n",
       "10867         ./../data/train/tt4151098/shot_0037.mp4         Static   \n",
       "1008   ./../data/trailer_zoom/tt2140577_shot_0019.mp4           Push   \n",
       "\n",
       "       movement_value  \n",
       "8768                4  \n",
       "3960                2  \n",
       "15303               0  \n",
       "9564                4  \n",
       "3113                2  \n",
       "10475               4  \n",
       "6692                1  \n",
       "4379                1  \n",
       "10867               4  \n",
       "1008                2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"finish_data_train.csv\")\n",
    "test_df = pd.read_csv(\"finish_data_test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:59.715795Z",
     "iopub.status.busy": "2022-07-15T13:47:59.714720Z",
     "iopub.status.idle": "2022-07-15T13:47:59.734565Z",
     "shell.execute_reply": "2022-07-15T13:47:59.732413Z",
     "shell.execute_reply.started": "2022-07-15T13:47:59.715739Z"
    },
    "id": "5Hw0DC-ToaeN"
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=MAX_SEQ_LENGTH):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    # Total frames\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    n_frames = round(length/max_frames)\n",
    "    \n",
    "    total = 0\n",
    "    frames = []\n",
    "    i=0\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or total == max_frames:\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if i%n_frames == 0:\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "                frames.append(frame)\n",
    "                total += 1\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "        if total < max_frames:\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            for j in range(int(length)):\n",
    "                ret, frame = cap.read()\n",
    "                if j == (int(length)-1):\n",
    "                    frame = crop_center_square(frame)\n",
    "                    frame = frame[:, :, [2, 1, 0]]\n",
    "                    frames.append(frame)\n",
    "            \n",
    "            \n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:47:59.759696Z",
     "iopub.status.busy": "2022-07-15T13:47:59.757049Z",
     "iopub.status.idle": "2022-07-15T13:48:01.692400Z",
     "shell.execute_reply": "2022-07-15T13:48:01.691315Z",
     "shell.execute_reply.started": "2022-07-15T13:47:59.759632Z"
    },
    "id": "adJtce-ylOEK"
   },
   "outputs": [],
   "source": [
    "# Xception model\n",
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.Xception(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "\n",
    "    preprocess_input = keras.applications.xception.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:48:01.694334Z",
     "iopub.status.busy": "2022-07-15T13:48:01.693993Z",
     "iopub.status.idle": "2022-07-15T13:48:01.708234Z",
     "shell.execute_reply": "2022-07-15T13:48:01.707262Z",
     "shell.execute_reply.started": "2022-07-15T13:48:01.694298Z"
    },
    "id": "N4iKANonlC2t",
    "outputId": "74df28f9-4a63-4e63-9ebc-3bc35e23b894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda) (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, 2048)              20861480  \n",
      "=================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:48:01.710841Z",
     "iopub.status.busy": "2022-07-15T13:48:01.710142Z",
     "iopub.status.idle": "2022-07-15T13:48:01.730342Z",
     "shell.execute_reply": "2022-07-15T13:48:01.728707Z",
     "shell.execute_reply.started": "2022-07-15T13:48:01.710797Z"
    },
    "id": "Ab99jJVCblE_",
    "outputId": "eaac8036-b9a1-43ed-d11c-65492b75ba50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motion', 'Pull', 'Push', 'Static']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"movement_label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:48:01.733634Z",
     "iopub.status.busy": "2022-07-15T13:48:01.732932Z",
     "iopub.status.idle": "2022-07-15T13:55:14.031487Z",
     "shell.execute_reply": "2022-07-15T13:55:14.030253Z",
     "shell.execute_reply.started": "2022-07-15T13:48:01.733582Z"
    },
    "id": "W9c3Esm7blE_",
    "outputId": "6fe49419-ae36-4c61-8d96-6b95a59e0a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (15844, 20, 2048)\n",
      "Frame masks in train set: (15844, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"dir\"].values.tolist()\n",
    "    labels = df[\"movement_label\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVtrrAMmblFA"
   },
   "source": [
    "## The sequence model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:55:14.035345Z",
     "iopub.status.busy": "2022-07-15T13:55:14.034418Z",
     "iopub.status.idle": "2022-07-15T13:55:35.035228Z",
     "shell.execute_reply": "2022-07-15T13:55:35.033905Z",
     "shell.execute_reply.started": "2022-07-15T13:55:14.035295Z"
    },
    "id": "kl3auqQLblFB",
    "outputId": "a413deeb-6a86-4982-9e31-cfe4de6530e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================] - 55s 136ms/step - loss: 1.3790 - accuracy: 0.3709 - val_loss: 1.4215 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42155, saving model to /tmp/video_classifier000\n",
      "Epoch 2/10\n",
      "347/347 [==============================] - 43s 123ms/step - loss: 1.3650 - accuracy: 0.3725 - val_loss: 1.4565 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42155\n",
      "Epoch 3/10\n",
      "347/347 [==============================] - 43s 123ms/step - loss: 1.3519 - accuracy: 0.3725 - val_loss: 1.4912 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.42155\n",
      "Epoch 4/10\n",
      "347/347 [==============================] - 42s 122ms/step - loss: 1.3396 - accuracy: 0.3725 - val_loss: 1.5258 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.42155\n",
      "Epoch 5/10\n",
      "347/347 [==============================] - 42s 122ms/step - loss: 1.3280 - accuracy: 0.3725 - val_loss: 1.5602 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42155\n",
      "Epoch 6/10\n",
      "347/347 [==============================] - 43s 124ms/step - loss: 1.3171 - accuracy: 0.3725 - val_loss: 1.5945 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42155\n",
      "Epoch 7/10\n",
      "347/347 [==============================] - 43s 124ms/step - loss: 1.3068 - accuracy: 0.3725 - val_loss: 1.6287 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42155\n",
      "Epoch 8/10\n",
      "347/347 [==============================] - 42s 122ms/step - loss: 1.2971 - accuracy: 0.3725 - val_loss: 1.6628 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.42155\n",
      "Epoch 9/10\n",
      "347/347 [==============================] - 43s 124ms/step - loss: 1.2879 - accuracy: 0.3725 - val_loss: 1.6967 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.42155\n",
      "Epoch 10/10\n",
      "347/347 [==============================] - 42s 121ms/step - loss: 1.2792 - accuracy: 0.3725 - val_loss: 1.7307 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.42155\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 1.3869 - accuracy: 0.2551\n",
      "Test accuracy: 25.51%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "\n",
    "    x = keras.layers.LSTM(256, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.LSTM(128)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x) # ver si al quitar mejora el modelo\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "    rnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier000\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-blVjAXblFB"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:20:33.165567Z",
     "iopub.status.busy": "2022-07-15T14:20:33.164850Z",
     "iopub.status.idle": "2022-07-15T14:20:33.831570Z",
     "shell.execute_reply": "2022-07-15T14:20:33.827030Z",
     "shell.execute_reply.started": "2022-07-15T14:20:33.165530Z"
    },
    "id": "q8-7Uuk0blFB",
    "outputId": "24bdc3eb-e07c-49a8-b2e3-01bf27021678"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"dir\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c75407a027d59d8279fdc80f39ae7e88eaf5822626513196156b9ba3e7422158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
